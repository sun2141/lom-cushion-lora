{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title LOM Cushion LoRA Trainer ğŸ§ \n",
    "\n",
    "# @markdown ë¨¼ì € í•„ìš”í•œ í™˜ê²½ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "# @markdown **ì‹¤í–‰ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”!**\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# ê¹ƒí—ˆë¸Œ ì €ì¥ì†Œì—ì„œ íŒŒì¼ ê°€ì ¸ì˜¤ê¸° ì„¤ì •\n",
    "github_repo = \"sun2141/lom-cushion-lora\"  # @param {type:\"string\"}\n",
    "\n",
    "# í™˜ê²½ í™•ì¸ ë° ì„¤ì •\n",
    "print(\"ğŸ” í™˜ê²½ í™•ì¸ ì¤‘...\")\n",
    "\n",
    "# CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸\n",
    "import torch\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "    print(f\"í˜„ì¬ CUDA ì¥ì¹˜: {torch.cuda.current_device()}\")\n",
    "    print(f\"ì¥ì¹˜ ì´ë¦„: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# ì €ì¥ì†Œ í´ë¡ \n",
    "print(\"\\nğŸ“¥ ê¹ƒí—ˆë¸Œ ì €ì¥ì†Œ í´ë¡  ì¤‘...\")\n",
    "!git clone https://github.com/{github_repo}.git lom_cushion_repo\n",
    "%cd lom_cushion_repo\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìºì‹œ ì •ë¦¬\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "print(\"\\nğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# ê°€ë” íŠ¹ì • ë²„ì „ ì¶©ëŒì´ ë°œìƒí•  ìˆ˜ ìˆì–´ numpyì™€ pandas ì¬ì„¤ì¹˜\n",
    "print(\"\\nğŸ”„ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜ ì¤‘...\")\n",
    "!pip install -q numpy>=1.26.4 --no-deps\n",
    "!pip install -q pandas==2.0.3 --no-deps\n",
    "\n",
    "print(\"\\nâœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
    "\n",
    "# Google Drive ì—°ê²° (ëª¨ë¸ ì €ì¥ìš©)\n",
    "# @markdown í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•  Google Driveë¥¼ ì—°ê²°í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\n",
    "connect_drive = True  # @param {type:\"boolean\"}\n",
    "\n",
    "if connect_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    save_directory = \"/content/drive/MyDrive/LOM_Cushion_LoRA\"\n",
    "    !mkdir -p {save_directory}\n",
    "    print(f\"âœ… Google Drive ì—°ê²° ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: {save_directory}\")\n",
    "else:\n",
    "    save_directory = \"/content/lom_cushion_output\"\n",
    "    !mkdir -p {save_directory}\n",
    "    print(f\"ğŸ“ ë¡œì»¬ ì €ì¥ ê²½ë¡œ: {save_directory}\")\n",
    "\n",
    "# ì„¸ì…˜ ì—°ê²° ìœ ì§€ ì„¤ì •\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "# @title ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì¤€ë¹„ ğŸ–¼ï¸\n",
    "\n",
    "# @markdown Hugging Faceì—ì„œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "dataset_name = \"sun2141/lom-cushion-images\"  # @param {type:\"string\"}\n",
    "\n",
    "print(\"ğŸ“¥ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "!huggingface-cli login --token hf_dummy_token_for_script_execution\n",
    "# Hugging Face ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "try:\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    print(f\"âœ… ë°ì´í„°ì…‹ '{dataset_name}' ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ëŒ€ì‹  ë¡œì»¬ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ë¡œì»¬ ì´ë¯¸ì§€ í´ë” í™•ì¸\n",
    "    if not os.path.exists(\"images\"):\n",
    "        print(\"âš ï¸ ë¡œì»¬ ì´ë¯¸ì§€ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. í´ë” ìƒì„± ì¤‘...\")\n",
    "        !mkdir -p images\n",
    "        print(\"images í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì— í•™ìŠµ ì´ë¯¸ì§€ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.\")\n",
    "    else:\n",
    "        print(\"ğŸ“ ë¡œì»¬ ì´ë¯¸ì§€ í´ë” í™•ì¸ ì™„ë£Œ!\")\n",
    "        image_count = len([f for f in os.listdir(\"images\") if os.path.isfile(os.path.join(\"images\", f))])\n",
    "        print(f\"ğŸ“Š ì´ë¯¸ì§€ íŒŒì¼ {image_count}ê°œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ìº¡ì…˜ íŒŒì¼ í™•ì¸\n",
    "print(\"\\nğŸ“ ìº¡ì…˜ íŒŒì¼ í™•ì¸ ì¤‘...\")\n",
    "\n",
    "prompts_dir = \"prompts\"\n",
    "if not os.path.exists(prompts_dir):\n",
    "    !mkdir -p {prompts_dir}\n",
    "    print(f\"ğŸ“ {prompts_dir} í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "caption_file = os.path.join(prompts_dir, \"image_caption.csv\")\n",
    "if not os.path.exists(caption_file):\n",
    "    print(f\"âš ï¸ ìº¡ì…˜ íŒŒì¼ {caption_file}ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ì´ë¯¸ ì œê³µëœ ìº¡ì…˜ ë‚´ìš©ì„ ì €ì¥\n",
    "    with open(caption_file, \"w\") as f:\n",
    "        f.write(\"filename,prompt\\n\")\n",
    "        # ìº¡ì…˜ íŒŒì¼ ë‚´ìš© ì¶”ê°€\n",
    "        with open(\"../image_caption.csv\", \"r\") as source:\n",
    "            next(source)  # í—¤ë” ê±´ë„ˆë›°ê¸°\n",
    "            for line in source:\n",
    "                f.write(line)\n",
    "    print(f\"âœ… {caption_file} íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\"âœ… ìº¡ì…˜ íŒŒì¼ {caption_file}ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "import pandas as pd\n",
    "try:\n",
    "    captions_df = pd.read_csv(caption_file)\n",
    "    print(f\"ğŸ“Š ìº¡ì…˜ íŒŒì¼ ì •ë³´: {len(captions_df)}ê°œì˜ í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ìº¡ì…˜ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "# @title ëª¨ë¸ ì„¤ì • ë° í•™ìŠµ íŒŒë¼ë¯¸í„° âš™ï¸\n",
    "\n",
    "# @markdown ê¸°ë³¸ ëª¨ë¸ê³¼ í•™ìŠµ ì„¤ì •ì„ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "# ê¸°ë³¸ ëª¨ë¸ ì„¤ì •\n",
    "pretrained_model = \"runwayml/stable-diffusion-v1-5\"  # @param {type:\"string\"}\n",
    "resolution = 512  # @param {type:\"integer\"}\n",
    "batch_size = 1  # @param {type:\"integer\"}\n",
    "num_train_epochs = 10  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-4  # @param {type:\"number\"}\n",
    "lora_r = 4  # @param {type:\"integer\"}\n",
    "lora_alpha = 32  # @param {type:\"integer\"}\n",
    "gradient_accumulation_steps = 4  # @param {type:\"integer\"}\n",
    "mixed_precision = \"fp16\"  # @param [\"no\", \"fp16\", \"bf16\"]\n",
    "train_text_encoder = True  # @param {type:\"boolean\"}\n",
    "checkpointing_steps = 500  # @param {type:\"integer\"}\n",
    "\n",
    "print(\"âš™ï¸ í•™ìŠµ ì„¤ì •:\")\n",
    "print(f\"- ê¸°ë³¸ ëª¨ë¸: {pretrained_model}\")\n",
    "print(f\"- í•´ìƒë„: {resolution} x {resolution}\")\n",
    "print(f\"- ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "print(f\"- í•™ìŠµ ì—í­: {num_train_epochs}\")\n",
    "print(f\"- í•™ìŠµë¥ : {learning_rate}\")\n",
    "print(f\"- LoRA ë­í¬ (r): {lora_r}\")\n",
    "print(f\"- LoRA ì•ŒíŒŒ: {lora_alpha}\")\n",
    "print(f\"- ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„: {gradient_accumulation_steps}\")\n",
    "print(f\"- í˜¼í•© ì •ë°€ë„: {mixed_precision}\")\n",
    "print(f\"- í…ìŠ¤íŠ¸ ì¸ì½”ë” í•™ìŠµ: {train_text_encoder}\")\n",
    "print(f\"- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë‹¨ê³„: {checkpointing_steps}\")\n",
    "\n",
    "# @title í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° ì‹¤í–‰ ğŸš€\n",
    "\n",
    "# @markdown í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.\n",
    "\n",
    "print(\"ğŸ”§ í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...\")\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "from diffusers.optimization import get_scheduler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì¤€ë¹„ í•¨ìˆ˜\n",
    "def prepare_dataset(image_folder, caption_file):\n",
    "    \"\"\"ì´ë¯¸ì§€ í´ë”ì™€ ìº¡ì…˜ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\"\"\"\n",
    "    import pandas as pd\n",
    "    from torch.utils.data import Dataset\n",
    "    from PIL import Image\n",
    "    \n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, image_folder, captions_df, tokenizer, size=512):\n",
    "            self.image_folder = image_folder\n",
    "            self.captions_df = captions_df\n",
    "            self.tokenizer = tokenizer\n",
    "            self.size = size\n",
    "            \n",
    "            # ì¡´ì¬í•˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ë§Œ í•„í„°ë§\n",
    "            valid_files = []\n",
    "            for idx, row in self.captions_df.iterrows():\n",
    "                file_path = os.path.join(image_folder, row['filename'])\n",
    "                if os.path.exists(file_path):\n",
    "                    valid_files.append(idx)\n",
    "            \n",
    "            self.valid_indices = valid_files\n",
    "            print(f\"ìœ íš¨í•œ ì´ë¯¸ì§€ íŒŒì¼: {len(self.valid_indices)}/{len(self.captions_df)}\")\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.valid_indices)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            idx = self.valid_indices[idx]\n",
    "            row = self.captions_df.iloc[idx]\n",
    "            \n",
    "            image_path = os.path.join(self.image_folder, row['filename'])\n",
    "            prompt = row['prompt']\n",
    "            \n",
    "            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\n",
    "            if image.width != self.size or image.height != self.size:\n",
    "                image = image.resize((self.size, self.size), Image.LANCZOS)\n",
    "            \n",
    "            # í† í°í™”\n",
    "            text_inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜\n",
    "            image = (torch.from_numpy(np.array(image)) / 255.0) * 2.0 - 1.0\n",
    "            image = image.permute(2, 0, 1).float()\n",
    "            \n",
    "            return {\n",
    "                \"pixel_values\": image,\n",
    "                \"input_ids\": text_inputs.input_ids[0],\n",
    "                \"attention_mask\": text_inputs.attention_mask[0],\n",
    "            }\n",
    "    \n",
    "    captions_df = pd.read_csv(caption_file)\n",
    "    print(f\"ìº¡ì…˜ íŒŒì¼ì—ì„œ {len(captions_df)}ê°œì˜ í•­ëª©ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    return captions_df, CustomDataset\n",
    "\n",
    "# ëª¨ë¸ ì¤€ë¹„ í•¨ìˆ˜\n",
    "def prepare_model(pretrained_model, lora_r, lora_alpha, train_text_encoder):\n",
    "    \"\"\"ëª¨ë¸ì„ ë¡œë“œí•˜ê³  LoRA ì„¤ì •ìœ¼ë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # íŒŒì´í”„ë¼ì¸ ë¡œë“œ\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        pretrained_model,\n",
    "        torch_dtype=torch.float16 if mixed_precision == \"fp16\" else torch.float32\n",
    "    )\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì¸ì½”ë”\n",
    "    text_encoder = pipeline.text_encoder\n",
    "    tokenizer = pipeline.tokenizer\n",
    "    \n",
    "    # U-Net\n",
    "    unet = pipeline.unet\n",
    "    \n",
    "    # LoRA êµ¬ì„±\n",
    "    if lora_r > 0:\n",
    "        # U-Netì— LoRA ì ìš©\n",
    "        unet_lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "            init_lora_weights=\"gaussian\",\n",
    "        )\n",
    "        unet = get_peft_model(unet, unet_lora_config)\n",
    "        unet.print_trainable_parameters()\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ì¸ì½”ë”ì—ë„ LoRA ì ìš© (ì˜µì…˜)\n",
    "        if train_text_encoder:\n",
    "            text_encoder_lora_config = LoraConfig(\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "                init_lora_weights=\"gaussian\",\n",
    "            )\n",
    "            text_encoder = get_peft_model(text_encoder, text_encoder_lora_config)\n",
    "            text_encoder.print_trainable_parameters()\n",
    "    \n",
    "    return pipeline, unet, text_encoder, tokenizer\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜\n",
    "def train_lora(unet, text_encoder, dataset, tokenizer, train_text_encoder, learning_rate, num_train_epochs, gradient_accumulation_steps):\n",
    "    \"\"\"ëª¨ë¸ í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # í•™ìŠµ ë§¤ê°œë³€ìˆ˜ ì„¤ì •\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    \n",
    "    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "    params_to_optimize = [\n",
    "        {\"params\": unet.parameters(), \"lr\": learning_rate},\n",
    "    ]\n",
    "    \n",
    "    if train_text_encoder:\n",
    "        params_to_optimize.append(\n",
    "            {\"params\": text_encoder.parameters(), \"lr\": learning_rate},\n",
    "        )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params_to_optimize,\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-8,\n",
    "    )\n",
    "    \n",
    "    # í•™ìŠµ ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"cosine\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=len(train_dataloader) * num_train_epochs // gradient_accumulation_steps,\n",
    "    )\n",
    "    \n",
    "    # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model, subfolder=\"scheduler\")\n",
    "    \n",
    "    # ì§„í–‰ìƒí™© ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜\n",
    "    global_step = 0\n",
    "    losses = []\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì¸ì½”ë”ì™€ U-Netì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    if train_text_encoder:\n",
    "        text_encoder.train()\n",
    "    unet.train()\n",
    "    \n",
    "    # í•™ìŠµ ë£¨í”„\n",
    "    progress_bar = tqdm(range(num_train_epochs * len(train_dataloader)), desc=\"Training\")\n",
    "    \n",
    "    for epoch in range(num_train_epochs):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„\n",
    "            pixel_values = batch[\"pixel_values\"].to(\"cuda\", non_blocking=True)\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\", non_blocking=True)\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ ì„ë² ë”©\n",
    "            with torch.no_grad():\n",
    "                if train_text_encoder:\n",
    "                    encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "                else:\n",
    "                    encoder_hidden_states = text_encoder(input_ids, output_hidden_states=True).hidden_states[-1]\n",
    "\n",
    "            # ë…¸ì´ì¦ˆ ì¶”ê°€ ë° ì ì¬ë³€ìˆ˜ ì¸ì½”ë”©\n",
    "            latents = pipeline.vae.encode(pixel_values).latent_dist.sample()\n",
    "            latents = latents * 0.18215\n",
    "            \n",
    "            # ë…¸ì´ì¦ˆ ìƒ˜í”Œë§\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "            timesteps = timesteps.long()\n",
    "            \n",
    "            # ë…¸ì´ì¦ˆ ì¶”ê°€\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            # U-Net ì˜ˆì¸¡\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            \n",
    "            # ì†ì‹¤ ê³„ì‚°\n",
    "            loss = torch.nn.functional.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "            \n",
    "            # ì†ì‹¤ ì—­ì „íŒŒ\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë° ì—…ë°ì´íŠ¸\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # ì†ì‹¤ ê¸°ë¡\n",
    "                losses.append(loss.detach().item() * gradient_accumulation_steps)\n",
    "                \n",
    "                # ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_postfix({\"loss\": losses[-1], \"epoch\": epoch + 1})\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥\n",
    "                if global_step % checkpointing_steps == 0:\n",
    "                    save_checkpoint(unet, text_encoder, tokenizer, epoch, global_step)\n",
    "            \n",
    "    # ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "    save_checkpoint(unet, text_encoder, tokenizer, num_train_epochs - 1, global_step, is_final=True)\n",
    "    \n",
    "    # ì†ì‹¤ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(f\"{save_directory}/training_loss.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í•¨ìˆ˜\n",
    "def save_checkpoint(unet, text_encoder, tokenizer, epoch, global_step, is_final=False):\n",
    "    \"\"\"ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if is_final:\n",
    "        checkpoint_dir = f\"{save_directory}/final_model\"\n",
    "    else:\n",
    "        checkpoint_dir = f\"{save_directory}/checkpoint-{global_step}\"\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # U-Net ìƒíƒœ ì €ì¥\n",
    "    unet.save_pretrained(checkpoint_dir)\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ì¸ì½”ë”ë„ ì €ì¥ (í•™ìŠµëœ ê²½ìš°)\n",
    "    if hasattr(text_encoder, \"save_pretrained\"):\n",
    "        text_encoder.save_pretrained(checkpoint_dir)\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì € ì €ì¥\n",
    "    tokenizer.save_pretrained(checkpoint_dir)\n",
    "    \n",
    "    print(f\"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {checkpoint_dir}\")\n",
    "\n",
    "# ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜\n",
    "def generate_sample_images(pipeline, prompt_examples=None, num_samples=4, guidance_scale=7.5):\n",
    "    \"\"\"ëª¨ë¸ë¡œ ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if prompt_examples is None:\n",
    "        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸\n",
    "        prompts = [\n",
    "            \"A cushion supporting arms while driving, realistic photo\",\n",
    "            \"A purple cushion with emerald pocket, product photo\",\n",
    "            \"A person using cushion while working on laptop, office setting\",\n",
    "            \"A cushion being used in car backseat, smartphone in pocket\"\n",
    "        ]\n",
    "    else:\n",
    "        # ì§€ì •ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©\n",
    "        prompts = prompt_examples[:num_samples]\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ìƒì„±\n",
    "    pipeline.to(\"cuda\")\n",
    "    pipeline.enable_attention_slicing()\n",
    "    \n",
    "    fig, axs = plt.subplots(1, len(prompts), figsize=(16, 4))\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        image = pipeline(prompt, num_inference_steps=30, guidance_scale=guidance_scale).images[0]\n",
    "        if len(prompts) > 1:\n",
    "            axs[i].imshow(image)\n",
    "            axs[i].set_title(prompt[:20] + \"...\", fontsize=10)\n",
    "            axs[i].axis(\"off\")\n",
    "        else:\n",
    "            axs.imshow(image)\n",
    "            axs.set_title(prompt[:20] + \"...\", fontsize=10)\n",
    "            axs.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_directory}/sample_generated_images.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± í•¨ìˆ˜\n",
    "def create_gradio_interface(pipeline):\n",
    "    \"\"\"Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    import gradio as gr\n",
    "    \n",
    "    def generate_image(prompt, guidance_scale=7.5, steps=30):\n",
    "        \"\"\"ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜\"\"\"\n",
    "        image = pipeline(\n",
    "            prompt,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=guidance_scale\n",
    "        ).images[0]\n",
    "        return image\n",
    "    \n",
    "    # ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ\n",
    "    example_prompts = []\n",
    "    example_file = os.path.join(\"prompts\", \"example_prompts.csv\")\n",
    "    if os.path.exists(example_file):\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(example_file)\n",
    "        example_prompts = df[\"prompt\"].tolist()\n",
    "    \n",
    "    # Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# LOM Cushion ì´ë¯¸ì§€ ìƒì„±ê¸°\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                prompt = gr.Textbox(label=\"í”„ë¡¬í”„íŠ¸\", placeholder=\"ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...\")\n",
    "                guidance = gr.Slider(minimum=1.0, maximum=15.0, value=7.5, step=0.5, label=\"Guidance Scale\")\n",
    "                steps = gr.Slider(minimum=10, maximum=100, value=30, step=1, label=\"ìƒì„± ë‹¨ê³„\")\n",
    "                btn = gr.Button(\"ì´ë¯¸ì§€ ìƒì„±í•˜ê¸°\")\n",
    "            \n",
    "            with gr.Column():\n",
    "                output = gr.Image(label=\"ìƒì„±ëœ ì´ë¯¸ì§€\")\n",
    "        \n",
    "        # ì˜ˆì‹œ ì¶”ê°€\n",
    "        if example_prompts:\n",
    "            gr.Examples(\n",
    "                examples=[[p] for p in example_prompts[:10]],\n",
    "                inputs=[prompt],\n",
    "                outputs=output,\n",
    "                fn=lambda p: generate_image(p),\n",
    "                cache_examples=True\n",
    "            )\n",
    "        \n",
    "        # ì´ë²¤íŠ¸ ì—°ê²°\n",
    "        btn.click(fn=generate_image, inputs=[prompt, guidance, steps], outputs=output)\n",
    "    \n",
    "    # ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰\n",
    "    demo.launch(share=True)\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "    # ì´ë¯¸ì§€ í´ë”ì™€ ìº¡ì…˜ íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "    image_folder = \"images\"\n",
    "    caption_file = os.path.join(\"prompts\", \"image_caption.csv\")\n",
    "\n",
    "    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    pipeline, unet, text_encoder, tokenizer = prepare_model(\n",
    "        pretrained_model, lora_r, lora_alpha, train_text_encoder\n",
    "    )\n",
    "\n",
    "    # ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "    captions_df, CustomDataset = prepare_dataset(image_folder, caption_file)\n",
    "    dataset = CustomDataset(image_folder, captions_df, tokenizer, size=resolution)\n",
    "\n",
    "    # í•™ìŠµ ì‹¤í–‰\n",
    "    unet.to(\"cuda\")\n",
    "    if train_text_encoder:\n",
    "        text_encoder.to(\"cuda\")\n",
    "\n",
    "    # í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰\n",
    "    losses = train_lora(\n",
    "        unet, text_encoder, dataset, tokenizer, \n",
    "        train_text_encoder, learning_rate, \n",
    "        num_train_epochs, gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    # ì €ì¥ëœ ëª¨ë¸ë¡œ íŒŒì´í”„ë¼ì¸ ì—…ë°ì´íŠ¸\n",
    "    pipeline.unet = unet.to(\"cuda\")\n",
    "    if train_text_encoder:\n",
    "        pipeline.text_encoder = text_encoder.to(\"cuda\")\n",
    "\n",
    "    # ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„±\n",
    "    print(\"\\nğŸ–¼ï¸ ìƒ˜í”Œ ì´ë¯¸ì§€ ìƒì„± ì¤‘...\")\n",
    "    \n",
    "    # ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ\n",
    "    example_prompts = []\n",
    "    example_file = os.path.join(\"prompts\", \"example_prompts.csv\")\n",
    "    if os.path.exists(example_file):\n",
    "        example_df = pd.read_csv(example_file)\n",
    "        example_prompts = example_df[\"prompt\"].tolist()\n",
    "    \n",
    "    generate_sample_images(pipeline, prompt_examples=example_prompts)\n",
    "\n",
    "    # Gradio ì¸í„°í˜ì´ìŠ¤ ì‹œì‘\n",
    "    print(\"\\nğŸŒ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹œì‘...\")\n",
    "    create_gradio_interface(pipeline)\n",
    "\n",
    "    print(\"\\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€ ì¶œë ¥\n",
    "    print(\"\\nğŸ“Š ì‹œìŠ¤í…œ ì •ë³´:\")\n",
    "    print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"í˜„ì¬ CUDA ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"ìµœëŒ€ CUDA ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
    "\n",
    "# @title Hugging Face ëª¨ë¸ í—ˆë¸Œì— ëª¨ë¸ ì—…ë¡œë“œ ğŸš€\n",
    "\n",
    "# @markdown í•™ìŠµì´ ì™„ë£Œëœ í›„ ëª¨ë¸ì„ Hugging Faceì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "\n",
    "# Hugging Face ì—…ë¡œë“œ ì„¤ì •\n",
    "upload_to_hub = False  # @param {type:\"boolean\"}\n",
    "hf_model_name = \"lom-cushion-lora\"  # @param {type:\"string\"}\n",
    "hf_token = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "if upload_to_hub:\n",
    "    from huggingface_hub import login, HfApi\n",
    "    \n",
    "    # í† í°ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸\n",
    "    if not hf_token:\n",
    "        print(\"âŒ Hugging Face í† í°ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í† í°ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Hugging Face ë¡œê·¸ì¸\n",
    "            login(token=hf_token, add_to_git_credential=True)\n",
    "            \n",
    "            # ìµœì¢… ëª¨ë¸ ê²½ë¡œ\n",
    "            model_path = f\"{save_directory}/final_model\"\n",
    "            \n",
    "            # ëª¨ë¸ ì„¤ëª… ìƒì„±\n",
    "            model_card = f\"\"\"\n",
    "            # LOM Cushion LoRA ëª¨ë¸\n",
    "            \n",
    "            ì´ ëª¨ë¸ì€ Stable Diffusion v1.5ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¿ ì…˜ ì´ë¯¸ì§€ ìƒì„±ì— íŠ¹í™”ëœ LoRA ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "            \n",
    "            ## ëª¨ë¸ ì •ë³´\n",
    "            - ê¸°ë³¸ ëª¨ë¸: {pretrained_model}\n",
    "            - LoRA ë­í¬ (r): {lora_r}\n",
    "            - LoRA ì•ŒíŒŒ: {lora_alpha}\n",
    "            - í•™ìŠµ ì—í­: {num_train_epochs}\n",
    "            \n",
    "            ## ì‚¬ìš© ë°©ë²•\n",
    "            ```python\n",
    "            from diffusers import StableDiffusionPipeline\n",
    "            import torch\n",
    "            \n",
    "            model_id = \"{hf_model_name}\"\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "            pipe.unet.load_attn_procs(model_id)\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "            \n",
    "            prompt = \"A cushion supporting arms while driving, realistic photo\"\n",
    "            image = pipe(prompt).images[0]\n",
    "            image.save(\"cushion.png\")\n",
    "            ```\n",
    "            \"\"\"\n",
    "            \n",
    "            # README.md íŒŒì¼ ìƒì„±\n",
    "            with open(f\"{model_path}/README.md\", \"w\") as f:\n",
    "                f.write(model_card)\n",
    "            \n",
    "            # ëª¨ë¸ ì—…ë¡œë“œ\n",
    "            print(f\"ğŸ“¤ ëª¨ë¸ì„ Hugging Face Hubì— ì—…ë¡œë“œ ì¤‘: {hf_model_name}\")\n",
    "            api = HfApi()\n",
    "            api.create_repo(repo_id=hf_model_name, exist_ok=True)\n",
    "            api.upload_folder(\n",
    "                folder_path=model_path,\n",
    "                repo_id=hf_model_name,\n",
    "                commit_message=\"Initial model upload\"\n",
    "            )\n",
    "            print(f\"âœ… ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ: https://huggingface.co/{hf_model_name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ëª¨ë¸ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ LOM Cushion LoRA Trainer ì‘ì—…ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
