{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title LOM Cushion LoRA Trainer 🧠\n",
    "\n",
    "# @markdown 먼저 필요한 환경을 설정합니다.\n",
    "# @markdown **실행 버튼을 누르세요!**\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# 깃허브 저장소에서 파일 가져오기 설정\n",
    "github_repo = \"sun2141/lom-cushion-lora\"  # @param {type:\"string\"}\n",
    "\n",
    "# 환경 확인 및 설정\n",
    "print(\"🔍 환경 확인 중...\")\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "import torch\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 버전: {torch.version.cuda}\")\n",
    "    print(f\"현재 CUDA 장치: {torch.cuda.current_device()}\")\n",
    "    print(f\"장치 이름: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# 저장소 클론\n",
    "print(\"\\n📥 깃허브 저장소 클론 중...\")\n",
    "!git clone https://github.com/{github_repo}.git lom_cushion_repo\n",
    "%cd lom_cushion_repo\n",
    "\n",
    "# 메모리 관리 및 캐시 정리\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# 필요한 패키지 설치\n",
    "print(\"\\n📦 필수 패키지 설치 중...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# 가끔 특정 버전 충돌이 발생할 수 있어 numpy와 pandas 재설치\n",
    "print(\"\\n🔄 호환성 문제 해결을 위한 패키지 재설치 중...\")\n",
    "!pip install -q numpy>=1.26.4 --no-deps\n",
    "!pip install -q pandas==2.0.3 --no-deps\n",
    "\n",
    "print(\"\\n✅ 환경 설정 완료!\")\n",
    "\n",
    "# Google Drive 연결 (모델 저장용)\n",
    "# @markdown 학습된 모델을 저장할 Google Drive를 연결하시겠습니까?\n",
    "connect_drive = True  # @param {type:\"boolean\"}\n",
    "\n",
    "if connect_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    save_directory = \"/content/drive/MyDrive/LOM_Cushion_LoRA\"\n",
    "    !mkdir -p {save_directory}\n",
    "    print(f\"✅ Google Drive 연결 완료! 저장 경로: {save_directory}\")\n",
    "else:\n",
    "    save_directory = \"/content/lom_cushion_output\"\n",
    "    !mkdir -p {save_directory}\n",
    "    print(f\"📁 로컬 저장 경로: {save_directory}\")\n",
    "\n",
    "# 세션 연결 유지 설정\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "# @title 데이터셋 다운로드 및 준비 🖼️\n",
    "\n",
    "# @markdown Hugging Face에서 데이터셋을 다운로드합니다.\n",
    "dataset_name = \"sun2141/lom-cushion-images\"  # @param {type:\"string\"}\n",
    "\n",
    "print(\"📥 데이터셋 다운로드 중...\")\n",
    "!huggingface-cli login --token hf_dummy_token_for_script_execution\n",
    "# Hugging Face 데이터셋 라이브러리 로드\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋 다운로드\n",
    "try:\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    print(f\"✅ 데이터셋 '{dataset_name}' 다운로드 완료!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 데이터셋 다운로드 실패: {e}\")\n",
    "    print(\"대신 로컬 이미지를 사용합니다.\")\n",
    "    \n",
    "    # 로컬 이미지 폴더 확인\n",
    "    if not os.path.exists(\"images\"):\n",
    "        print(\"⚠️ 로컬 이미지 폴더가 없습니다. 폴더 생성 중...\")\n",
    "        !mkdir -p images\n",
    "        print(\"images 폴더를 생성했습니다. 여기에 학습 이미지를 추가해주세요.\")\n",
    "    else:\n",
    "        print(\"📁 로컬 이미지 폴더 확인 완료!\")\n",
    "        image_count = len([f for f in os.listdir(\"images\") if os.path.isfile(os.path.join(\"images\", f))])\n",
    "        print(f\"📊 이미지 파일 {image_count}개를 발견했습니다.\")\n",
    "\n",
    "# 캡션 파일 확인\n",
    "print(\"\\n📝 캡션 파일 확인 중...\")\n",
    "\n",
    "prompts_dir = \"prompts\"\n",
    "if not os.path.exists(prompts_dir):\n",
    "    !mkdir -p {prompts_dir}\n",
    "    print(f\"📁 {prompts_dir} 폴더를 생성했습니다.\")\n",
    "\n",
    "caption_file = os.path.join(prompts_dir, \"image_caption.csv\")\n",
    "if not os.path.exists(caption_file):\n",
    "    print(f\"⚠️ 캡션 파일 {caption_file}이 없습니다.\")\n",
    "    \n",
    "    # 이미 제공된 캡션 내용을 저장\n",
    "    with open(caption_file, \"w\") as f:\n",
    "        f.write(\"filename,prompt\\n\")\n",
    "        # 캡션 파일 내용 추가\n",
    "        with open(\"../image_caption.csv\", \"r\") as source:\n",
    "            next(source)  # 헤더 건너뛰기\n",
    "            for line in source:\n",
    "                f.write(line)\n",
    "    print(f\"✅ {caption_file} 파일을 생성했습니다.\")\n",
    "else:\n",
    "    print(f\"✅ 캡션 파일 {caption_file}을 찾았습니다.\")\n",
    "\n",
    "import pandas as pd\n",
    "try:\n",
    "    captions_df = pd.read_csv(caption_file)\n",
    "    print(f\"📊 캡션 파일 정보: {len(captions_df)}개의 항목이 있습니다.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 캡션 파일 읽기 실패: {e}\")\n",
    "\n",
    "# @title 모델 설정 및 학습 파라미터 ⚙️\n",
    "\n",
    "# @markdown 기본 모델과 학습 설정을 구성합니다.\n",
    "\n",
    "# 기본 모델 설정\n",
    "pretrained_model = \"runwayml/stable-diffusion-v1-5\"  # @param {type:\"string\"}\n",
    "resolution = 512  # @param {type:\"integer\"}\n",
    "batch_size = 1  # @param {type:\"integer\"}\n",
    "num_train_epochs = 10  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-4  # @param {type:\"number\"}\n",
    "lora_r = 4  # @param {type:\"integer\"}\n",
    "lora_alpha = 32  # @param {type:\"integer\"}\n",
    "gradient_accumulation_steps = 4  # @param {type:\"integer\"}\n",
    "mixed_precision = \"fp16\"  # @param [\"no\", \"fp16\", \"bf16\"]\n",
    "train_text_encoder = True  # @param {type:\"boolean\"}\n",
    "checkpointing_steps = 500  # @param {type:\"integer\"}\n",
    "\n",
    "print(\"⚙️ 학습 설정:\")\n",
    "print(f\"- 기본 모델: {pretrained_model}\")\n",
    "print(f\"- 해상도: {resolution} x {resolution}\")\n",
    "print(f\"- 배치 크기: {batch_size}\")\n",
    "print(f\"- 학습 에폭: {num_train_epochs}\")\n",
    "print(f\"- 학습률: {learning_rate}\")\n",
    "print(f\"- LoRA 랭크 (r): {lora_r}\")\n",
    "print(f\"- LoRA 알파: {lora_alpha}\")\n",
    "print(f\"- 그래디언트 누적 단계: {gradient_accumulation_steps}\")\n",
    "print(f\"- 혼합 정밀도: {mixed_precision}\")\n",
    "print(f\"- 텍스트 인코더 학습: {train_text_encoder}\")\n",
    "print(f\"- 체크포인트 저장 단계: {checkpointing_steps}\")\n",
    "\n",
    "# @title 학습 파이프라인 구성 및 실행 🚀\n",
    "\n",
    "# @markdown 학습을 시작하려면 실행 버튼을 누르세요.\n",
    "\n",
    "print(\"🔧 학습 파이프라인 구성 중...\")\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "from diffusers.optimization import get_scheduler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# 데이터셋 준비 함수\n",
    "def prepare_dataset(image_folder, caption_file):\n",
    "    \"\"\"이미지 폴더와 캡션 파일을 기반으로 데이터셋을 준비합니다.\"\"\"\n",
    "    import pandas as pd\n",
    "    from torch.utils.data import Dataset\n",
    "    from PIL import Image\n",
    "    \n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, image_folder, captions_df, tokenizer, size=512):\n",
    "            self.image_folder = image_folder\n",
    "            self.captions_df = captions_df\n",
    "            self.tokenizer = tokenizer\n",
    "            self.size = size\n",
    "            \n",
    "            # 존재하는 이미지 파일만 필터링\n",
    "            valid_files = []\n",
    "            for idx, row in self.captions_df.iterrows():\n",
    "                file_path = os.path.join(image_folder, row['filename'])\n",
    "                if os.path.exists(file_path):\n",
    "                    valid_files.append(idx)\n",
    "            \n",
    "            self.valid_indices = valid_files\n",
    "            print(f\"유효한 이미지 파일: {len(self.valid_indices)}/{len(self.captions_df)}\")\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.valid_indices)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            idx = self.valid_indices[idx]\n",
    "            row = self.captions_df.iloc[idx]\n",
    "            \n",
    "            image_path = os.path.join(self.image_folder, row['filename'])\n",
    "            prompt = row['prompt']\n",
    "            \n",
    "            # 이미지 로드 및 전처리\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            # 이미지 크기 조정\n",
    "            if image.width != self.size or image.height != self.size:\n",
    "                image = image.resize((self.size, self.size), Image.LANCZOS)\n",
    "            \n",
    "            # 토큰화\n",
    "            text_inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            # 이미지를 텐서로 변환\n",
    "            image = (torch.from_numpy(np.array(image)) / 255.0) * 2.0 - 1.0\n",
    "            image = image.permute(2, 0, 1).float()\n",
    "            \n",
    "            return {\n",
    "                \"pixel_values\": image,\n",
    "                \"input_ids\": text_inputs.input_ids[0],\n",
    "                \"attention_mask\": text_inputs.attention_mask[0],\n",
    "            }\n",
    "    \n",
    "    captions_df = pd.read_csv(caption_file)\n",
    "    print(f\"캡션 파일에서 {len(captions_df)}개의 항목을 로드했습니다.\")\n",
    "    \n",
    "    return captions_df, CustomDataset\n",
    "\n",
    "# 모델 준비 함수\n",
    "def prepare_model(pretrained_model, lora_r, lora_alpha, train_text_encoder):\n",
    "    \"\"\"모델을 로드하고 LoRA 설정으로 준비합니다.\"\"\"\n",
    "    # 파이프라인 로드\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        pretrained_model,\n",
    "        torch_dtype=torch.float16 if mixed_precision == \"fp16\" else torch.float32\n",
    "    )\n",
    "    \n",
    "    # 텍스트 인코더\n",
    "    text_encoder = pipeline.text_encoder\n",
    "    tokenizer = pipeline.tokenizer\n",
    "    \n",
    "    # U-Net\n",
    "    unet = pipeline.unet\n",
    "    \n",
    "    # LoRA 구성\n",
    "    if lora_r > 0:\n",
    "        # U-Net에 LoRA 적용\n",
    "        unet_lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "            init_lora_weights=\"gaussian\",\n",
    "        )\n",
    "        unet = get_peft_model(unet, unet_lora_config)\n",
    "        unet.print_trainable_parameters()\n",
    "        \n",
    "        # 텍스트 인코더에도 LoRA 적용 (옵션)\n",
    "        if train_text_encoder:\n",
    "            text_encoder_lora_config = LoraConfig(\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "                init_lora_weights=\"gaussian\",\n",
    "            )\n",
    "            text_encoder = get_peft_model(text_encoder, text_encoder_lora_config)\n",
    "            text_encoder.print_trainable_parameters()\n",
    "    \n",
    "    return pipeline, unet, text_encoder, tokenizer\n",
    "\n",
    "# 학습 함수\n",
    "def train_lora(unet, text_encoder, dataset, tokenizer, train_text_encoder, learning_rate, num_train_epochs, gradient_accumulation_steps):\n",
    "    \"\"\"모델 학습을 실행합니다.\"\"\"\n",
    "    # 학습 매개변수 설정\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    \n",
    "    # 옵티마이저 설정\n",
    "    params_to_optimize = [\n",
    "        {\"params\": unet.parameters(), \"lr\": learning_rate},\n",
    "    ]\n",
    "    \n",
    "    if train_text_encoder:\n",
    "        params_to_optimize.append(\n",
    "            {\"params\": text_encoder.parameters(), \"lr\": learning_rate},\n",
    "        )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        params_to_optimize,\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-8,\n",
    "    )\n",
    "    \n",
    "    # 학습 스케줄러\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"cosine\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=len(train_dataloader) * num_train_epochs // gradient_accumulation_steps,\n",
    "    )\n",
    "    \n",
    "    # 노이즈 스케줄러\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model, subfolder=\"scheduler\")\n",
    "    \n",
    "    # 진행상황 추적을 위한 변수\n",
    "    global_step = 0\n",
    "    losses = []\n",
    "    \n",
    "    # 텍스트 인코더와 U-Net을 학습 모드로 설정\n",
    "    if train_text_encoder:\n",
    "        text_encoder.train()\n",
    "    unet.train()\n",
    "    \n",
    "    # 학습 루프\n",
    "    progress_bar = tqdm(range(num_train_epochs * len(train_dataloader)), desc=\"Training\")\n",
    "    \n",
    "    for epoch in range(num_train_epochs):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # 배치 데이터 준비\n",
    "            pixel_values = batch[\"pixel_values\"].to(\"cuda\", non_blocking=True)\n",
    "            input_ids = batch[\"input_ids\"].to(\"cuda\", non_blocking=True)\n",
    "            \n",
    "            # 텍스트 임베딩\n",
    "            with torch.no_grad():\n",
    "                if train_text_encoder:\n",
    "                    encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "                else:\n",
    "                    encoder_hidden_states = text_encoder(input_ids, output_hidden_states=True).hidden_states[-1]\n",
    "\n",
    "            # 노이즈 추가 및 잠재변수 인코딩\n",
    "            latents = pipeline.vae.encode(pixel_values).latent_dist.sample()\n",
    "            latents = latents * 0.18215\n",
    "            \n",
    "            # 노이즈 샘플링\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "            timesteps = timesteps.long()\n",
    "            \n",
    "            # 노이즈 추가\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "            \n",
    "            # U-Net 예측\n",
    "            noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            \n",
    "            # 손실 계산\n",
    "            loss = torch.nn.functional.mse_loss(noise_pred, noise, reduction=\"mean\")\n",
    "            \n",
    "            # 손실 역전파\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # 그래디언트 누적 및 업데이트\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 손실 기록\n",
    "                losses.append(loss.detach().item() * gradient_accumulation_steps)\n",
    "                \n",
    "                # 진행상황 업데이트\n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_postfix({\"loss\": losses[-1], \"epoch\": epoch + 1})\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # 체크포인트 저장\n",
    "                if global_step % checkpointing_steps == 0:\n",
    "                    save_checkpoint(unet, text_encoder, tokenizer, epoch, global_step)\n",
    "            \n",
    "    # 최종 모델 저장\n",
    "    save_checkpoint(unet, text_encoder, tokenizer, num_train_epochs - 1, global_step, is_final=True)\n",
    "    \n",
    "    # 손실 그래프 그리기\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.savefig(f\"{save_directory}/training_loss.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 체크포인트 저장 함수\n",
    "def save_checkpoint(unet, text_encoder, tokenizer, epoch, global_step, is_final=False):\n",
    "    \"\"\"모델 체크포인트를 저장합니다.\"\"\"\n",
    "    if is_final:\n",
    "        checkpoint_dir = f\"{save_directory}/final_model\"\n",
    "    else:\n",
    "        checkpoint_dir = f\"{save_directory}/checkpoint-{global_step}\"\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # U-Net 상태 저장\n",
    "    unet.save_pretrained(checkpoint_dir)\n",
    "    \n",
    "    # 텍스트 인코더도 저장 (학습된 경우)\n",
    "    if hasattr(text_encoder, \"save_pretrained\"):\n",
    "        text_encoder.save_pretrained(checkpoint_dir)\n",
    "    \n",
    "    # 토크나이저 저장\n",
    "    tokenizer.save_pretrained(checkpoint_dir)\n",
    "    \n",
    "    print(f\"체크포인트 저장 완료: {checkpoint_dir}\")\n",
    "\n",
    "# 이미지 생성 함수\n",
    "def generate_sample_images(pipeline, prompt_examples=None, num_samples=4, guidance_scale=7.5):\n",
    "    \"\"\"모델로 샘플 이미지를 생성합니다.\"\"\"\n",
    "    if prompt_examples is None:\n",
    "        # 기본 프롬프트\n",
    "        prompts = [\n",
    "            \"A cushion supporting arms while driving, realistic photo\",\n",
    "            \"A purple cushion with emerald pocket, product photo\",\n",
    "            \"A person using cushion while working on laptop, office setting\",\n",
    "            \"A cushion being used in car backseat, smartphone in pocket\"\n",
    "        ]\n",
    "    else:\n",
    "        # 지정된 프롬프트 사용\n",
    "        prompts = prompt_examples[:num_samples]\n",
    "    \n",
    "    # 이미지 생성\n",
    "    pipeline.to(\"cuda\")\n",
    "    pipeline.enable_attention_slicing()\n",
    "    \n",
    "    fig, axs = plt.subplots(1, len(prompts), figsize=(16, 4))\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        image = pipeline(prompt, num_inference_steps=30, guidance_scale=guidance_scale).images[0]\n",
    "        if len(prompts) > 1:\n",
    "            axs[i].imshow(image)\n",
    "            axs[i].set_title(prompt[:20] + \"...\", fontsize=10)\n",
    "            axs[i].axis(\"off\")\n",
    "        else:\n",
    "            axs.imshow(image)\n",
    "            axs.set_title(prompt[:20] + \"...\", fontsize=10)\n",
    "            axs.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{save_directory}/sample_generated_images.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Gradio 인터페이스 생성 함수\n",
    "def create_gradio_interface(pipeline):\n",
    "    \"\"\"Gradio 웹 인터페이스를 생성합니다.\"\"\"\n",
    "    import gradio as gr\n",
    "    \n",
    "    def generate_image(prompt, guidance_scale=7.5, steps=30):\n",
    "        \"\"\"이미지 생성 함수\"\"\"\n",
    "        image = pipeline(\n",
    "            prompt,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=guidance_scale\n",
    "        ).images[0]\n",
    "        return image\n",
    "    \n",
    "    # 예시 프롬프트 로드\n",
    "    example_prompts = []\n",
    "    example_file = os.path.join(\"prompts\", \"example_prompts.csv\")\n",
    "    if os.path.exists(example_file):\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(example_file)\n",
    "        example_prompts = df[\"prompt\"].tolist()\n",
    "    \n",
    "    # Gradio 인터페이스 구성\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# LOM Cushion 이미지 생성기\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                prompt = gr.Textbox(label=\"프롬프트\", placeholder=\"이미지를 설명하는 텍스트를 입력하세요...\")\n",
    "                guidance = gr.Slider(minimum=1.0, maximum=15.0, value=7.5, step=0.5, label=\"Guidance Scale\")\n",
    "                steps = gr.Slider(minimum=10, maximum=100, value=30, step=1, label=\"생성 단계\")\n",
    "                btn = gr.Button(\"이미지 생성하기\")\n",
    "            \n",
    "            with gr.Column():\n",
    "                output = gr.Image(label=\"생성된 이미지\")\n",
    "        \n",
    "        # 예시 추가\n",
    "        if example_prompts:\n",
    "            gr.Examples(\n",
    "                examples=[[p] for p in example_prompts[:10]],\n",
    "                inputs=[prompt],\n",
    "                outputs=output,\n",
    "                fn=lambda p: generate_image(p),\n",
    "                cache_examples=True\n",
    "            )\n",
    "        \n",
    "        # 이벤트 연결\n",
    "        btn.click(fn=generate_image, inputs=[prompt, guidance, steps], outputs=output)\n",
    "    \n",
    "    # 인터페이스 실행\n",
    "    demo.launch(share=True)\n",
    "\n",
    "# 학습 실행\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "try:\n",
    "    print(\"🚀 학습 시작...\")\n",
    "\n",
    "    # 이미지 폴더와 캡션 파일 경로 설정\n",
    "    image_folder = \"images\"\n",
    "    caption_file = os.path.join(\"prompts\", \"image_caption.csv\")\n",
    "\n",
    "    # 모델 및 토크나이저 로드\n",
    "    pipeline, unet, text_encoder, tokenizer = prepare_model(\n",
    "        pretrained_model, lora_r, lora_alpha, train_text_encoder\n",
    "    )\n",
    "\n",
    "    # 데이터셋 준비\n",
    "    captions_df, CustomDataset = prepare_dataset(image_folder, caption_file)\n",
    "    dataset = CustomDataset(image_folder, captions_df, tokenizer, size=resolution)\n",
    "\n",
    "    # 학습 실행\n",
    "    unet.to(\"cuda\")\n",
    "    if train_text_encoder:\n",
    "        text_encoder.to(\"cuda\")\n",
    "\n",
    "    # 학습 프로세스 실행\n",
    "    losses = train_lora(\n",
    "        unet, text_encoder, dataset, tokenizer, \n",
    "        train_text_encoder, learning_rate, \n",
    "        num_train_epochs, gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    # 저장된 모델로 파이프라인 업데이트\n",
    "    pipeline.unet = unet.to(\"cuda\")\n",
    "    if train_text_encoder:\n",
    "        pipeline.text_encoder = text_encoder.to(\"cuda\")\n",
    "\n",
    "    # 샘플 이미지 생성\n",
    "    print(\"\\n🖼️ 샘플 이미지 생성 중...\")\n",
    "    \n",
    "    # 예시 프롬프트 로드\n",
    "    example_prompts = []\n",
    "    example_file = os.path.join(\"prompts\", \"example_prompts.csv\")\n",
    "    if os.path.exists(example_file):\n",
    "        example_df = pd.read_csv(example_file)\n",
    "        example_prompts = example_df[\"prompt\"].tolist()\n",
    "    \n",
    "    generate_sample_images(pipeline, prompt_examples=example_prompts)\n",
    "\n",
    "    # Gradio 인터페이스 시작\n",
    "    print(\"\\n🌐 Gradio 인터페이스 시작...\")\n",
    "    create_gradio_interface(pipeline)\n",
    "\n",
    "    print(\"\\n✅ 모든 작업이 완료되었습니다!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ 오류 발생: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # 시스템 정보 추가 출력\n",
    "    print(\"\\n📊 시스템 정보:\")\n",
    "    print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"현재 CUDA 메모리 사용량: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"최대 CUDA 메모리 사용량: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n",
    "\n",
    "# @title Hugging Face 모델 허브에 모델 업로드 🚀\n",
    "\n",
    "# @markdown 학습이 완료된 후 모델을 Hugging Face에 업로드합니다.\n",
    "\n",
    "# Hugging Face 업로드 설정\n",
    "upload_to_hub = False  # @param {type:\"boolean\"}\n",
    "hf_model_name = \"lom-cushion-lora\"  # @param {type:\"string\"}\n",
    "hf_token = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "if upload_to_hub:\n",
    "    from huggingface_hub import login, HfApi\n",
    "    \n",
    "    # 토큰이 비어있지 않은지 확인\n",
    "    if not hf_token:\n",
    "        print(\"❌ Hugging Face 토큰이 비어있습니다. 토큰을 입력해주세요.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Hugging Face 로그인\n",
    "            login(token=hf_token, add_to_git_credential=True)\n",
    "            \n",
    "            # 최종 모델 경로\n",
    "            model_path = f\"{save_directory}/final_model\"\n",
    "            \n",
    "            # 모델 설명 생성\n",
    "            model_card = f\"\"\"\n",
    "            # LOM Cushion LoRA 모델\n",
    "            \n",
    "            이 모델은 Stable Diffusion v1.5를 기반으로 쿠션 이미지 생성에 특화된 LoRA 모델입니다.\n",
    "            \n",
    "            ## 모델 정보\n",
    "            - 기본 모델: {pretrained_model}\n",
    "            - LoRA 랭크 (r): {lora_r}\n",
    "            - LoRA 알파: {lora_alpha}\n",
    "            - 학습 에폭: {num_train_epochs}\n",
    "            \n",
    "            ## 사용 방법\n",
    "            ```python\n",
    "            from diffusers import StableDiffusionPipeline\n",
    "            import torch\n",
    "            \n",
    "            model_id = \"{hf_model_name}\"\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
    "            pipe.unet.load_attn_procs(model_id)\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "            \n",
    "            prompt = \"A cushion supporting arms while driving, realistic photo\"\n",
    "            image = pipe(prompt).images[0]\n",
    "            image.save(\"cushion.png\")\n",
    "            ```\n",
    "            \"\"\"\n",
    "            \n",
    "            # README.md 파일 생성\n",
    "            with open(f\"{model_path}/README.md\", \"w\") as f:\n",
    "                f.write(model_card)\n",
    "            \n",
    "            # 모델 업로드\n",
    "            print(f\"📤 모델을 Hugging Face Hub에 업로드 중: {hf_model_name}\")\n",
    "            api = HfApi()\n",
    "            api.create_repo(repo_id=hf_model_name, exist_ok=True)\n",
    "            api.upload_folder(\n",
    "                folder_path=model_path,\n",
    "                repo_id=hf_model_name,\n",
    "                commit_message=\"Initial model upload\"\n",
    "            )\n",
    "            print(f\"✅ 모델 업로드 완료: https://huggingface.co/{hf_model_name}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 모델 업로드 중 오류 발생: {e}\")\n",
    "\n",
    "print(\"\\n🎉 LOM Cushion LoRA Trainer 작업이 모두 완료되었습니다!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
