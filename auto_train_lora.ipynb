{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ LOM Cushion LoRA Trainer (Colab 노트북)\n",
    "\n",
    "# === [1] 환경설정 ===\n",
    "!pip install -U --force-reinstall numpy\n",
    "!pip uninstall -y bitsandbytes\n",
    "!pip install -q xformers==0.0.30.dev1005 diffusers==0.26.3 transformers==4.41.1 accelerate==0.30.1 safetensors==0.4.2 peft==0.10.0 huggingface_hub==0.25.1 gradio==4.24.0\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === [2] 데이터셋 다운로드 ===\n",
    "!git clone https://huggingface.co/datasets/sun2141/lom-cushion-images\n",
    "\n",
    "from pathlib import Path\n",
    "image_dir = Path(\"lom-cushion-images/images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === [3] 데이터셋 준비 ===\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "caption_file = Path(\"lom-cushion-images/prompts/image_caption.csv\")\n",
    "captions_df = pd.read_csv(caption_file)\n",
    "\n",
    "class CushionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir):\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(self.image_dir / row[\"filename\"]).convert(\"RGB\").resize((512, 512))\n",
    "        image_np = np.array(image)\n",
    "        image_tensor = torch.tensor(image_np).permute(2, 0, 1).float() / 255.0\n",
    "        return image_tensor, row[\"prompt\"]\n",
    "\n",
    "train_dataset = CushionDataset(captions_df, image_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === [4] LoRA 훈련 ===\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.nn.functional import mse_loss\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "base_model = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.float16)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(base_model)\n",
    "text_encoder = pipeline.text_encoder\n",
    "unet = pipeline.unet\n",
    "scheduler = DDPMScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=[\"attn2.to_q\", \"attn2.to_k\", \"attn2.to_v\", \"attn2.to_out.0\"])\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for images, prompts in train_loader:\n",
    "        text_inputs = tokenizer(prompts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        encoder_hidden_states = text_encoder(text_inputs.input_ids.to(images.device))[0]\n",
    "\n",
    "        noise = torch.randn_like(images)\n",
    "        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (1,), device=images.device).long()\n",
    "        noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "        noise_pred = unet(noisy_images, timesteps, encoder_hidden_states).sample\n",
    "        loss = mse_loss(noise_pred, noise)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb62601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === [5] Hugging Face 업로드 ===\n",
    "unet.save_pretrained(\"./lora-cushion\")\n",
    "!huggingface-cli upload ./lora-cushion \"sun2141/lora-cushion\" --repo-type model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa217b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === [6] Gradio 이미지 생성 ===\n",
    "import gradio as gr\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "def generate(prompt):\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\"sun2141/lora-cushion\", torch_dtype=torch.float16)\n",
    "    return pipe(prompt).images[0]\n",
    "\n",
    "gr.Interface(fn=generate, inputs=\"text\", outputs=\"image\").launch(share=True)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
