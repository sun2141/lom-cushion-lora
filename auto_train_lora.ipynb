{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376dacee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… LOM Cushion LoRA Trainer (Colab ë…¸íŠ¸ë¶)\n",
    "\n",
    "# === [1] í™˜ê²½ì„¤ì • ===\n",
    "# Colab ê¸°ë³¸ torch==2.7.0, torchvision==0.22.0, torchaudio==2.7.0 ìœ ì§€\n",
    "!pip install -U --force-reinstall numpy  # numpy ê°•ì œ ì¬ì„¤ì¹˜ (ë°”ì´ë„ˆë¦¬ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)\n",
    "!pip uninstall -y bitsandbytes  # bitsandbytes ì œê±° (triton ì¶©ëŒ ë°©ì§€)\n",
    "!pip install -q xformers==0.0.30.dev1005 diffusers==0.26.3 transformers==4.41.1 accelerate==0.30.1 safetensors==0.4.2 peft==0.10.0 huggingface_hub==0.25.1 gradio==4.24.0\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()  # ğŸ¤— Hugging Face ë¡œê·¸ì¸ (í† í° í•„ìš”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2aaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === [2] Hugging Face Datasetì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ===\n",
    "!git clone https://huggingface.co/datasets/sun2141/lom-cushion-images\n",
    "\n",
    "from pathlib import Path\n",
    "image_dir = Path(\"lom-cushion-images/images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf68975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === [3] ë°ì´í„°ì…‹ ì¤€ë¹„ ===\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "caption_file = Path(\"lom-cushion-images/prompts/image_caption.csv\")\n",
    "captions_df = pd.read_csv(caption_file)\n",
    "\n",
    "class CushionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir):\n",
    "        self.df = dataframe\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(self.image_dir / row[\"filename\"]).convert(\"RGB\").resize((512, 512))\n",
    "        image_np = np.array(image)\n",
    "        image_tensor = torch.tensor(image_np).permute(2, 0, 1).float() / 255.0\n",
    "        return image_tensor, row[\"prompt\"]\n",
    "\n",
    "train_dataset = CushionDataset(captions_df, image_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === [4] LoRA í›ˆë ¨ ì¤€ë¹„ ë° ì‹¤í–‰ ===\n",
    "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "base_model = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.float16)\n",
    "text_encoder = pipeline.text_encoder\n",
    "unet = pipeline.unet\n",
    "scheduler = DDPMScheduler.from_config(pipeline.scheduler.config)\n",
    "\n",
    "target_modules = [\"attn2.to_q\", \"attn2.to_k\", \"attn2.to_v\", \"attn2.to_out.0\"]\n",
    "lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=target_modules)\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for images, prompts in train_loader:\n",
    "        noise = torch.randn_like(images)\n",
    "        timesteps = torch.randint(0, scheduler.num_train_timesteps, (1,), device=images.device).long()\n",
    "        noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "\n",
    "        encoder_hidden_states = text_encoder(prompts).last_hidden_state\n",
    "        noise_pred = unet(noisy_images, timesteps, encoder_hidden_states).sample\n",
    "        loss = mse_loss(noise_pred, noise)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9abe4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === [5] Hugging Face ì—…ë¡œë“œ ===\n",
    "unet.save_pretrained(\"./lora-cushion\")\n",
    "!huggingface-cli upload ./lora-cushion \"sun2141/lora-cushion\" --repo-type model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === [6] Gradio ì´ë¯¸ì§€ ìƒì„± ===\n",
    "import gradio as gr\n",
    "\n",
    "def generate(prompt):\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\"sun2141/lora-cushion\", torch_dtype=torch.float16)\n",
    "    return pipe(prompt).images[0]\n",
    "\n",
    "gr.Interface(fn=generate, inputs=\"text\", outputs=\"image\").launch(share=True)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
